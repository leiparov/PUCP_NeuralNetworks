{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA usando SVD\n",
    "\n",
    "Antes de abordar la implementación de autoencoders para el aprendizaje de representaciones partamos con recordar el tipo de representaciones que podemos aprender con un método analítico lineal como el análisis de componentes principales (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Importar librerías que usaremos más adelante\n",
    "#\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Cargar y visualizar el conjunto de datos que emplearemos\n",
    "#\n",
    "ex7data1 = loadmat('ex7data1.mat')\n",
    "X = np.array(ex7data1['X'])\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.scatter(X[:,0], X[:,1], marker='o', cmap='prism')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de aplicar PCA es importante normalizar los datos. Los centraremos para tener una media $\\mu \\approx 0$ y varianza $\\sigma^2 \\approx 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(X, axis=0)\n",
    "sigma = np.std(X, axis=0)\n",
    "\n",
    "print('Antes de normalizar')\n",
    "print('mu    = {}'.format(mu))\n",
    "print('sigma = {}'.format(sigma))\n",
    "print('\\nPrimeros 5 puntos:\\n', X[:5])\n",
    "\n",
    "X_norm = (X - mu) / sigma\n",
    "\n",
    "mu_norm = np.mean(X_norm, axis=0)\n",
    "sigma_norm = np.std(X_norm, axis=0)\n",
    "\n",
    "print('\\n\\nDespués de normalizar')\n",
    "print('mu    = {}'.format(mu_norm))\n",
    "print('sigma = {}'.format(sigma_norm))\n",
    "print('\\nPrimeros 5 puntos:\\n', X_norm[:5])\n",
    "\n",
    "#\n",
    "# Cargar y visualizar el conjunto de datos que emplearemos\n",
    "#\n",
    "ex7data1 = loadmat('ex7data1.mat')\n",
    "X = np.array(ex7data1['X'])\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.scatter(X_norm[:,0], X_norm[:,1], marker='o', cmap='prism')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando descomposición de valores singulares obtenemos las matrices $U$, $\\Sigma$ y $V^\\top$ tales que:\n",
    "\n",
    "$$X = U \\Sigma V^\\top$$\n",
    "\n",
    "* Las filas de $V^\\top$ corresponden a los **vectores de cargas** de los componentes principales de $X$.\n",
    "* Las columnas de $U \\Sigma$ (o, lo que es lo mismo, $XV$) corresponden a los **vectores de puntajes** de los componentes principales de $X$.\n",
    "* $\\Sigma$ es una matriz diagonal que contiene la **varianza** $\\sigma^2$ en cada uno de los ejes de los componentes principales de $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# SVD\n",
    "#\n",
    "(U, s, Vt) = np.linalg.svd(X_norm)\n",
    "\n",
    "#\n",
    "# Vectores de cargas\n",
    "#\n",
    "pca_loadings = Vt\n",
    "\n",
    "print('El vector de cargas del primer componente principal es: {} ({:.1f}°)'.format(pca_loadings[0,:],\n",
    "                                                                               np.arctan(pca_loadings[0,0] / pca_loadings[0,1]) * 360 / (2 * np.pi)))\n",
    "print('El vector de cargas del primer componente principal es: {} ({:.1f}°)'.format(pca_loadings[1,:],\n",
    "                                                                               np.arctan(pca_loadings[1,0] / pca_loadings[1,1]) * 360 / (2 * np.pi)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Reconstruimos Sigma para calcular la varianza explicada por cada componente\n",
    "#\n",
    "explained_variance = (s * s) / X_norm.shape[0]\n",
    "\n",
    "print('Varianza explicada por cada componente principal:')\n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Visualizamos los datos normalizados y los vectores de cargas\n",
    "#\n",
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "plt.scatter(X_norm[:,0], X_norm[:,1], marker='o', cmap='prism')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "a = 1.2\n",
    "plt.annotate('PCA1', (pca_loadings[0,0]*explained_variance[0]*a, pca_loadings[0,1]*explained_variance[0]*a), color='blue', ha='right')\n",
    "plt.arrow(0,0, pca_loadings[0,0]*explained_variance[0], pca_loadings[0,1]*explained_variance[0], color='b', head_width=0.1, length_includes_head=True)\n",
    "plt.annotate('PCA2', (pca_loadings[1,0]*explained_variance[1]*a, pca_loadings[1,1]*explained_variance[1]*a), color='blue', ha='right')\n",
    "plt.arrow(0,0, pca_loadings[1,0]*explained_variance[1], pca_loadings[1,1]*explained_variance[1], color='b', head_width=0.1, length_includes_head=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proyección de los datos sobre los componentes principales\n",
    "\n",
    "Después de calcular los componentes principales, puedes usarlos para reducir la dimensión de características de tu conjunto de datos proyectando cada ejemplo sobre un espacio de menores dimensiones, $x^{(i)} \\rightarrow z^{(i)}$ (p.ej., proyectando los datos de 2D a 1D).\n",
    "\n",
    "Para proyectar los datos sólo necesitas truncar las primeras `K` columnas de la matriz de puntajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Función para proyectar los datos sobre los primeros K componentes\n",
    "#\n",
    "def projectData(pca_loadings, X, K):\n",
    "  V = pca_loadings.T         # Recuperamos V a partir de pca_loadings = V.T\n",
    "  pca_scores = np.dot(X, V)  # Puntajes para todos los componentes\n",
    "  Z = pca_scores[:,:K]       # Puntajes para los primeros K componentes\n",
    "  return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Proyectar los datos\n",
    "#\n",
    "Z = projectData(pca_loadings, X_norm, 1)\n",
    "\n",
    "print('La proyección del primer ejemplo sobre la primera dimensión es: {}'.format(Z[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstrucción de los datos a partir de la proyección\n",
    "\n",
    "Luego de proyectar los datos en un espacio de menores dimensiones, puedes recuperar aproximadamente los datos proyectándolos de regreso en el espacio original de alta dimensión. Para hacerlo, multiplicamos la proyección Z por las primeras `K` filas de la matriz de carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Función para reconstuir los datos a partir de la proyección Z\n",
    "#\n",
    "def recoverData(pca_loadings, Z):\n",
    "    K = Z.shape[1]\n",
    "    X_rec = np.dot(Z, pca_loadings[:K,:])\n",
    "    return X_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Reconstuir los datos\n",
    "#\n",
    "X_norm_rec = recoverData(pca_loadings, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Función para calcular el error cuadrático medio\n",
    "#\n",
    "def MSE(X, X_rec):\n",
    "  return np.mean(np.sum((X - X_rec)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Calcular el error de reconstrucción\n",
    "#\n",
    "error_de_reconstruccion = MSE(X_norm, X_norm_rec)\n",
    "print('Error de reconstrucción (MSE) de los datos normalizados : {:.7f}'.format(error_de_reconstruccion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de las proyecciones\n",
    "\n",
    "Una vez realizada la proyección y reconstrucción aproximada de los datos, veamos en el siguiente diagrama cómo la proyección afecta los datos. Los puntos de datos originales están indicados con color azul, mientras que los puntos de datos proyectados están indicados con color rojo. La proyección retiene efectivamente la información en la dirección del primer componente principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "for i in range(X_norm.shape[0]):\n",
    "    plt.arrow(X_norm[i,0], X_norm[i,1], X_norm_rec[i,0]-X_norm[i,0], X_norm_rec[i,1]-X_norm[i,1], color='k')\n",
    "plt.scatter(X_norm[:,0], X_norm[:,1], marker='o', c='blue', label='Datos originales')  \n",
    "plt.scatter(X_norm_rec[:,0], X_norm_rec[:,1], marker='o', c='red', label='Datos recuperados')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA usando un autoencoder lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si construimos un autoencoder lineal a minimizar el error cuadrático medio, el autoencoder aprenderá a describir el mismo supespacio lineal que PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptodo de https://github.com/ageron/handson-ml/blob/master/15_autoencoders.ipynb\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#\n",
    "# Dimensiones del modelo\n",
    "#\n",
    "n_inputs  = X.shape[1]    # 2D\n",
    "n_hidden  = 1             # 1D\n",
    "n_outputs = n_inputs\n",
    "\n",
    "#\n",
    "# Modelo\n",
    "#\n",
    "x = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden = tf.layers.dense(x, n_hidden, activation=None)\n",
    "outputs = tf.layers.dense(hidden, n_outputs, activation=None)\n",
    "\n",
    "#\n",
    "# Función de pérdida (MSE) y entrenamiento\n",
    "#\n",
    "mse_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - outputs), axis=1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.1)\n",
    "training_op = optimizer.minimize(mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  tf.global_variables_initializer().run()\n",
    "  for iteration in range(n_iterations):\n",
    "    _, mse = sess.run([training_op, mse_loss],\n",
    "                           feed_dict={x: X_norm})\n",
    "    if iteration % 100 == 0:\n",
    "        print('Iteración: {:04d}, MSE: {:.9f}'.format(iteration, mse))\n",
    "\n",
    "  codings, X_rec_nn = sess.run([hidden, outputs], feed_dict={x: X_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error de reconstrucción (MSE) con PCA analítico : {:.7f}'.format(error_de_reconstruccion))\n",
    "print('Error de reconstrucción (MSE) con PCA autoencoder : {:.7f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "for i in range(X_norm.shape[0]):\n",
    "    plt.arrow(X_norm[i,0], X_norm[i,1], X_rec_nn[i,0]-X_norm[i,0], X_rec_nn[i,1]-X_norm[i,1], color='k')\n",
    "plt.scatter(X_norm[:,0], X_norm[:,1], marker='o', c='blue', label='Datos originales')  \n",
    "plt.scatter(X_rec_nn[:,0], X_rec_nn[:,1], marker='o', c='red', label='Datos recuperados con autoencoder')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Generalización de PCA usando un autoencoder no lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#\n",
    "# Dimensiones del modelo\n",
    "#\n",
    "n_inputs  = X.shape[1]    # 2D\n",
    "n_hidden1 = 2\n",
    "n_hidden2 = 1             # Codings\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "#\n",
    "# Modelo\n",
    "#\n",
    "x = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden1 = tf.layers.dense(x, n_hidden1, activation=tf.nn.sigmoid)\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.sigmoid)\n",
    "hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.sigmoid)\n",
    "outputs = tf.layers.dense(hidden3, n_outputs, activation=None)\n",
    "\n",
    "#\n",
    "# Función de pérdida (MSE) y entrenamiento\n",
    "#\n",
    "mse_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - outputs), axis=1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.15)\n",
    "training_op = optimizer.minimize(mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 2001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  tf.global_variables_initializer().run()\n",
    "  for iteration in range(n_iterations):\n",
    "    _, mse = sess.run([training_op, mse_loss],\n",
    "                           feed_dict={x: X_norm})\n",
    "    if iteration % 100 == 0:\n",
    "        print('Iteración: {:04d}, MSE: {:.9f}'.format(iteration, mse))\n",
    "\n",
    "  codings, X_rec_nn = sess.run([hidden2, outputs], feed_dict={x: X_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Error de reconstrucción (MSE) con PCA analítico         : {:.7f}'.format(error_de_reconstruccion))\n",
    "print('Error de reconstrucción (MSE) con autoencoder no lineal : {:.7f}'.format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axes().set_aspect('equal', 'datalim')\n",
    "#for i in range(X_norm.shape[0]):\n",
    "#    plt.arrow(X_norm[i,0], X_norm[i,1], X_rec_nn[i,0]-X_norm[i,0], X_rec_nn[i,1]-X_norm[i,1], color='k')\n",
    "plt.scatter(X_norm[:,0], X_norm[:,1], marker='o', c='blue', label='Datos originales')  \n",
    "plt.scatter(X_rec_nn[:,0], X_rec_nn[:,1], marker='o', c='red', label='Datos recuperados con autoencoder')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA estándar de 30 dimensiones para MNIST\n",
    "\n",
    "En adelante usaremos MNIST para visualizar la calidad de las representaciones obtenidas. Empecemos por establecer una línea base con PCA estándar de 30 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Proyección y reconstrución de MNIST usando PCA de 30 dimensiones\n",
    "#\n",
    "X = mnist.train.images[:5000]   # Si usamos las 55000 recibimos un error de memoria (!)\n",
    "(U, s, Vt) = np.linalg.svd(X)\n",
    "Z = projectData(Vt, X, 30)      # Proyectamos los datos a los primeros 30 componentes principales...\n",
    "R = recoverData(Vt, Z)          # ...y reconstruimos los datos a su espacio original\n",
    "error_de_reconstruccion = MSE(X, R)\n",
    "print('Error de reconstrucción (MSE) de los datos normalizados : {:.7f}'.format(error_de_reconstruccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Función de ayuda para visualizar las imágenes\n",
    "#\n",
    "from matplotlib.pyplot import figure, imshow, axis\n",
    "\n",
    "def mnist_grid(X):\n",
    "  X = X.reshape([-1, 28, 28])\n",
    "  num_images = X.shape[0]\n",
    "  fig = figure()\n",
    "  fig.set_size_inches(10, 10)\n",
    "\n",
    "  for i in range(num_images):\n",
    "      a=fig.add_subplot(1, num_images, i +1)\n",
    "      image = X[i,:]\n",
    "      imshow(image,cmap='gray')\n",
    "      axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualicemos las primeras 10 imágenes originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_grid(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualicemos su reconstrucción a partir de los primeros 30 componentes de PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_grid(R[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autoencoder profundo subcompleto 784-256-128-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#\n",
    "# Dimensiones del modelo\n",
    "#\n",
    "n_inputs  = X.shape[1]    # 28 x 28\n",
    "n_hidden1 = 256\n",
    "n_hidden2 = 128\n",
    "n_hidden3 = 32             # Codings\n",
    "n_hidden4 = n_hidden2\n",
    "n_hidden5 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "#\n",
    "# Modelo\n",
    "#\n",
    "x = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden1 = tf.layers.dense(x, n_hidden1, activation=tf.nn.sigmoid)\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.sigmoid)\n",
    "hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.sigmoid)\n",
    "hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.sigmoid)\n",
    "hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.sigmoid)\n",
    "outputs = tf.layers.dense(hidden5, n_outputs, activation=None)\n",
    "\n",
    "#\n",
    "# Función de pérdida (MSE) y entrenamiento\n",
    "#\n",
    "mse_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - outputs), axis=1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  tf.global_variables_initializer().run()\n",
    "  for iteration in range(n_iterations):\n",
    "    _, mse = sess.run([training_op, mse_loss],\n",
    "                           feed_dict={x: X})\n",
    "    if iteration % 100 == 0:\n",
    "        print('Iteración: {:04d}, MSE: {:.9f}'.format(iteration, mse))\n",
    "        R = sess.run(outputs, feed_dict={x: X[:10]})\n",
    "        mnist_grid(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autoencoder disperso sobrecompleto 784-1000-784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#\n",
    "# Dimensiones del modelo\n",
    "#\n",
    "n_inputs = X.shape[1]    # 28 x 28\n",
    "n_hidden1 = 1000         # Codings\n",
    "n_outputs = n_inputs\n",
    "\n",
    "sparsity_target = 0.032\n",
    "sparsity_weight = 0.2\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    # Kullback Leibler divergence\n",
    "    return p * tf.log(p / q) + (1 - p) * tf.log((1 - p) / (1 - q))\n",
    "\n",
    "#\n",
    "# Modelo\n",
    "#\n",
    "x = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden1 = tf.layers.dense(x, n_hidden1, activation=tf.nn.sigmoid)\n",
    "outputs = tf.layers.dense(hidden1, n_outputs, activation=None)\n",
    "\n",
    "#\n",
    "# Función de pérdida (MSE) y entrenamiento\n",
    "#\n",
    "hidden1_mean = tf.reduce_mean(hidden1, axis=0) # batch mean\n",
    "sparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target, hidden1_mean))\n",
    "mse_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - outputs), axis=1))\n",
    "loss = mse_loss + sparsity_weight * sparsity_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  tf.global_variables_initializer().run()\n",
    "  for iteration in range(n_iterations):\n",
    "    _, mse = sess.run([training_op, mse_loss],\n",
    "                           feed_dict={x: X})\n",
    "    if iteration % 100 == 0:\n",
    "        print('Iteración: {:04d}, MSE: {:.9f}'.format(iteration, mse))\n",
    "        R = sess.run(outputs, feed_dict={x: X[:10]})\n",
    "        mnist_grid(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Denoising autoencoder 784-256-128-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#\n",
    "# Dimensiones del modelo\n",
    "#\n",
    "n_inputs  = X.shape[1]    # 28 x 28\n",
    "n_hidden1 = 256\n",
    "n_hidden2 = 128\n",
    "n_hidden3 = 32             # Codings\n",
    "n_hidden4 = n_hidden2\n",
    "n_hidden5 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "noise_level = 1.0\n",
    "\n",
    "#\n",
    "# Modelo\n",
    "#\n",
    "x = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "x_noisy = x + noise_level * tf.random_normal(tf.shape(x))\n",
    "hidden1 = tf.layers.dense(x_noisy, n_hidden1, activation=tf.nn.sigmoid)\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.sigmoid)\n",
    "hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.sigmoid)\n",
    "hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.sigmoid)\n",
    "hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.sigmoid)\n",
    "outputs = tf.layers.dense(hidden5, n_outputs, activation=None)\n",
    "\n",
    "#\n",
    "# Función de pérdida (MSE) y entrenamiento\n",
    "#\n",
    "mse_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - outputs), axis=1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  tf.global_variables_initializer().run()\n",
    "  for iteration in range(n_iterations):\n",
    "    _, mse = sess.run([training_op, mse_loss],\n",
    "                           feed_dict={x: X})\n",
    "    if iteration % 100 == 0:\n",
    "        print('Iteración: {:04d}, MSE: {:.9f}'.format(iteration, mse))\n",
    "        R = sess.run(outputs, feed_dict={x: X[:10]})\n",
    "        mnist_grid(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Variational autoencoder 784-256-256-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#\n",
    "# Dimensiones del modelo\n",
    "#\n",
    "n_inputs  = X.shape[1]    # 28 x 28\n",
    "n_hidden1 = 256\n",
    "n_hidden2 = 256\n",
    "n_hidden3 = 32             # Codings\n",
    "n_hidden4 = n_hidden2\n",
    "n_hidden5 = n_hidden1\n",
    "n_outputs = n_inputs\n",
    "\n",
    "#\n",
    "# Modelo\n",
    "#\n",
    "x = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "hidden1 = tf.layers.dense(x, n_hidden1, activation=tf.nn.elu)\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.elu)\n",
    "hidden3_mean = tf.layers.dense(hidden2, n_hidden3, activation=None)\n",
    "hidden3_sigma = tf.layers.dense(hidden2, n_hidden3, activation=None)\n",
    "noise = tf.random_normal(tf.shape(hidden3_sigma), dtype=tf.float32)\n",
    "hidden3 = hidden3_mean + hidden3_sigma * noise\n",
    "hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.elu)\n",
    "hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.elu)\n",
    "logits = tf.layers.dense(hidden5, n_outputs, activation=None)\n",
    "outputs = tf.sigmoid(logits)\n",
    "\n",
    "#\n",
    "# Función de pérdida (MSE) y entrenamiento\n",
    "#\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=logits)\n",
    "xentropy_loss = tf.reduce_sum(xentropy)\n",
    "eps = 1e-10 # smoothing term to avoid computing log(0) which is NaN\n",
    "latent_loss = 0.5 * tf.reduce_sum(\n",
    "    tf.square(hidden3_sigma) + tf.square(hidden3_mean)\n",
    "    - 1 - tf.log(eps + tf.square(hidden3_sigma)))\n",
    "loss = xentropy_loss + latent_loss\n",
    "\n",
    "mse_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - outputs), axis=1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 2001\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  tf.global_variables_initializer().run()\n",
    "  for iteration in range(n_iterations):\n",
    "    _, mse = sess.run([training_op, mse_loss],\n",
    "                           feed_dict={x: X})\n",
    "    if iteration % 100 == 0:\n",
    "        print('Iteración: {:04d}, MSE: {:.9f}'.format(iteration, mse))\n",
    "        R = sess.run(outputs, feed_dict={x: X[:10]})\n",
    "        mnist_grid(R)\n",
    "        \n",
    "  saver.save(sess, \"./VAE.ckpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_digits = 10\n",
    "random_codings = np.random.normal(size=[n_digits, n_hidden3])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  saver.restore(sess, \"./VAE.ckpt\")\n",
    "  generated_images = sess.run(outputs, feed_dict={hidden3: random_codings})\n",
    "\n",
    "mnist_grid(generated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
